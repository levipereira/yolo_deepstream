bash scripts/eval-trt.sh experiments/qat/20240206202935/qat_best_5108.pt data/coco.yaml  experiments/eval_trt



========== Diagnostic Run torch.onnx.export version 1.14.0a0+44dac51 ===========
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================

Save onnx to experiments/eval_trt/qat_best_5108.onnx


&&&& RUNNING TensorRT.trtexec [TensorRT v8503] # trtexec --onnx=experiments/eval_trt/qat_best_5108.onnx --saveEngine=experiments/eval_trt/qat_best_5108.engine --fp16 --int8 --buildOnly --memPoolSize=workspace:1024MiB --dumpLayerInfo --exportLayerInfo=experiments/eval_trt/qat_best_5108.graph --profilingVerbosity=detailed
[02/06/2024-23:15:52] [I] === Model Options ===
[02/06/2024-23:15:52] [I] Format: ONNX
[02/06/2024-23:15:52] [I] Model: experiments/eval_trt/qat_best_5108.onnx
[02/06/2024-23:15:52] [I] Output:
[02/06/2024-23:15:52] [I] === Build Options ===
[02/06/2024-23:15:52] [I] Max batch: explicit batch
[02/06/2024-23:15:52] [I] Memory Pools: workspace: 1024 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[02/06/2024-23:15:52] [I] minTiming: 1
[02/06/2024-23:15:52] [I] avgTiming: 8
[02/06/2024-23:15:52] [I] Precision: FP32+FP16+INT8
[02/06/2024-23:15:52] [I] LayerPrecisions:
[02/06/2024-23:15:52] [I] Calibration: Dynamic
[02/06/2024-23:15:52] [I] Refit: Disabled
[02/06/2024-23:15:52] [I] Sparsity: Disabled
[02/06/2024-23:15:52] [I] Safe mode: Disabled
[02/06/2024-23:15:52] [I] DirectIO mode: Disabled
[02/06/2024-23:15:52] [I] Restricted mode: Disabled
[02/06/2024-23:15:52] [I] Build only: Enabled
[02/06/2024-23:15:52] [I] Save engine: experiments/eval_trt/qat_best_5108.engine
[02/06/2024-23:15:52] [I] Load engine:
[02/06/2024-23:15:52] [I] Profiling verbosity: 2
[02/06/2024-23:15:52] [I] Tactic sources: Using default tactic sources
[02/06/2024-23:15:52] [I] timingCacheMode: local
[02/06/2024-23:15:52] [I] timingCacheFile:
[02/06/2024-23:15:52] [I] Heuristic: Disabled
[02/06/2024-23:15:52] [I] Preview Features: Use default preview flags.
[02/06/2024-23:15:52] [I] Input(s)s format: fp32:CHW
[02/06/2024-23:15:52] [I] Output(s)s format: fp32:CHW
[02/06/2024-23:15:52] [I] Input build shapes: model
[02/06/2024-23:15:52] [I] Input calibration shapes: model
[02/06/2024-23:15:52] [I] === System Options ===
[02/06/2024-23:15:52] [I] Device: 0
[02/06/2024-23:15:52] [I] DLACore:
[02/06/2024-23:15:52] [I] Plugins:
[02/06/2024-23:15:52] [I] === Inference Options ===
[02/06/2024-23:15:52] [I] Batch: Explicit
[02/06/2024-23:15:52] [I] Input inference shapes: model
[02/06/2024-23:15:52] [I] Iterations: 10
[02/06/2024-23:15:52] [I] Duration: 3s (+ 200ms warm up)
[02/06/2024-23:15:52] [I] Sleep time: 0ms
[02/06/2024-23:15:52] [I] Idle time: 0ms
[02/06/2024-23:15:52] [I] Streams: 1
[02/06/2024-23:15:52] [I] ExposeDMA: Disabled
[02/06/2024-23:15:52] [I] Data transfers: Enabled
[02/06/2024-23:15:52] [I] Spin-wait: Disabled
[02/06/2024-23:15:52] [I] Multithreading: Disabled
[02/06/2024-23:15:52] [I] CUDA Graph: Disabled
[02/06/2024-23:15:52] [I] Separate profiling: Disabled
[02/06/2024-23:15:52] [I] Time Deserialize: Disabled
[02/06/2024-23:15:52] [I] Time Refit: Disabled
[02/06/2024-23:15:52] [I] NVTX verbosity: 2
[02/06/2024-23:15:52] [I] Persistent Cache Ratio: 0
[02/06/2024-23:15:52] [I] Inputs:
[02/06/2024-23:15:52] [I] === Reporting Options ===
[02/06/2024-23:15:52] [I] Verbose: Disabled
[02/06/2024-23:15:52] [I] Averages: 10 inferences
[02/06/2024-23:15:52] [I] Percentiles: 90,95,99
[02/06/2024-23:15:52] [I] Dump refittable layers:Disabled
[02/06/2024-23:15:52] [I] Dump output: Disabled
[02/06/2024-23:15:52] [I] Profile: Disabled
[02/06/2024-23:15:52] [I] Export timing to JSON file:
[02/06/2024-23:15:52] [I] Export output to JSON file:
[02/06/2024-23:15:52] [I] Export profile to JSON file:
[02/06/2024-23:15:52] [I]
[02/06/2024-23:15:52] [I] === Device Information ===
[02/06/2024-23:15:52] [I] Selected Device: NVIDIA GeForce RTX 4090
[02/06/2024-23:15:52] [I] Compute Capability: 8.9
[02/06/2024-23:15:52] [I] SMs: 128
[02/06/2024-23:15:52] [I] Compute Clock Rate: 2.58 GHz
[02/06/2024-23:15:52] [I] Device Global Memory: 24215 MiB
[02/06/2024-23:15:52] [I] Shared Memory per SM: 100 KiB
[02/06/2024-23:15:52] [I] Memory Bus Width: 384 bits (ECC disabled)
[02/06/2024-23:15:52] [I] Memory Clock Rate: 10.501 GHz
[02/06/2024-23:15:52] [I]
[02/06/2024-23:15:52] [I] TensorRT version: 8.5.3
[02/06/2024-23:15:52] [I] [TRT] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 26, GPU 421 (MiB)
[02/06/2024-23:15:54] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +546, GPU +118, now: CPU 627, GPU 539 (MiB)
[02/06/2024-23:15:54] [I] Start parsing network model
[02/06/2024-23:15:54] [I] [TRT] ----------------------------------------------------------------
[02/06/2024-23:15:54] [I] [TRT] Input filename:   experiments/eval_trt/qat_best_5108.onnx
[02/06/2024-23:15:54] [I] [TRT] ONNX IR version:  0.0.7
[02/06/2024-23:15:54] [I] [TRT] Opset version:    13
[02/06/2024-23:15:54] [I] [TRT] Producer name:    pytorch
[02/06/2024-23:15:54] [I] [TRT] Producer version: 1.14.0
[02/06/2024-23:15:54] [I] [TRT] Domain:
[02/06/2024-23:15:54] [I] [TRT] Model version:    0
[02/06/2024-23:15:54] [I] [TRT] Doc string:
[02/06/2024-23:15:54] [I] [TRT] ----------------------------------------------------------------
[02/06/2024-23:15:55] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[02/06/2024-23:15:55] [I] Finish parsing network model
[02/06/2024-23:15:55] [W] Dynamic dimensions required for input: images, but no shapes were provided. Automatically overriding shape to: 1x3x672x672
[02/06/2024-23:15:55] [W] [TRT] Calibrator won't be used in explicit precision mode. Use quantization aware training to generate network with Quantize/Dequantize nodes.
[02/06/2024-23:16:00] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +6, GPU +10, now: CPU 919, GPU 555 (MiB)
[02/06/2024-23:16:00] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 921, GPU 565 (MiB)
[02/06/2024-23:16:00] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[02/06/2024-23:18:44] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
[02/06/2024-23:22:33] [I] [TRT] Total Activation Memory: 1244035584
[02/06/2024-23:22:33] [I] [TRT] Detected 1 inputs and 4 output network tensors.
[02/06/2024-23:22:34] [I] [TRT] Total Host Persistent Memory: 284448
[02/06/2024-23:22:34] [I] [TRT] Total Device Persistent Memory: 0
[02/06/2024-23:22:34] [I] [TRT] Total Scratch Memory: 0
[02/06/2024-23:22:34] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 183 MiB, GPU 239 MiB
[02/06/2024-23:22:34] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 120 steps to complete.
[02/06/2024-23:22:34] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 4.90347ms to assign 10 blocks to 120 nodes requiring 34292736 bytes.
[02/06/2024-23:22:34] [I] [TRT] Total Activation Memory: 34292736
[02/06/2024-23:22:34] [W] [TRT] TensorRT encountered issues when converting weights between types and that could affect accuracy.
[02/06/2024-23:22:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.
[02/06/2024-23:22:34] [W] [TRT] Check verbose logs for the list of affected weights.
[02/06/2024-23:22:34] [W] [TRT] - 3 weights are affected by this issue: Detected subnormal FP16 values.
[02/06/2024-23:22:34] [W] [TRT] - 85 weights are affected by this issue: Detected values which are outside of int8_t range and clipped them to int8_t range.
[02/06/2024-23:22:34] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +36, GPU +37, now: CPU 36, GPU 37 (MiB)
[02/06/2024-23:22:34] [I] Engine built in 402.276 sec.
[02/06/2024-23:22:34] [I] [TRT] Loaded engine size: 38 MiB
[02/06/2024-23:22:35] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +36, now: CPU 0, GPU 36 (MiB)
[02/06/2024-23:22:35] [I] Engine deserialized in 0.0226676 sec.
[02/06/2024-23:22:35] [I] Layer Information:
[02/06/2024-23:22:35] [I] Layers:
Name: /model.105/Constant_6_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1091) [Constant]_output, Location: Device, Dimensions: [1,1,84,84,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Constant, weights: {"Type": "Float", "Count": 14112}, dimensions: [1,1,84,84,2], TacticValue: 0x0000000000000000
Name: /model.105/Constant_8_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1096) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Constant, weights: {"Type": "Float", "Count": 6}, dimensions: [1,3,1,1,2], TacticValue: 0x0000000000000000
Name: /model.105/Constant_17_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1140) [Constant]_output, Location: Device, Dimensions: [1,1,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: Constant, weights: {"Type": "Float", "Count": 3528}, dimensions: [1,1,42,42,2], TacticValue: 0x0000000000000000
Name: /model.105/Constant_19_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1145) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP32 }], ParameterType: Constant, weights: {"Type": "Float", "Count": 6}, dimensions: [1,3,1,1,2], TacticValue: 0x0000000000000000
Name: /model.105/Constant_28_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1189) [Constant]_output, Location: Device, Dimensions: [1,1,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: Constant, weights: {"Type": "Float", "Count": 882}, dimensions: [1,1,21,21,2], TacticValue: 0x0000000000000000
Name: /model.105/Constant_30_output_0, LayerType: Constant, Inputs: [], Outputs: [ { Name: (Unnamed Layer* 1194) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP32 }], ParameterType: Constant, weights: {"Type": "Float", "Count": 6}, dimensions: [1,3,1,1,2], TacticValue: 0x0000000000000000
Name: /model.0/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,672,672], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.0/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,672,672], Format/Datatype: Four wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000
Name: Reformatting CopyNode for Input Tensor 0 to model.0.conv.weight + /model.0/conv/_weight_quantizer/QuantizeLinear + /model.0/conv/Conv + PWN(/model.0/act/Sigmoid, /model.0/act/Mul), LayerType: Reformat, Inputs: [ { Name: /model.0/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,672,672], Format/Datatype: Four wide channel vectorized row major Int8 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to model.0.conv.weight + /model.0/conv/_weight_quantizer/QuantizeLinear + /model.0/conv/Conv + PWN(/model.0/act/Sigmoid, /model.0/act/Mul), Location: Device, Dimensions: [1,3,672,672], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: model.0.conv.weight + /model.0/conv/_weight_quantizer/QuantizeLinear + /model.0/conv/Conv + PWN(/model.0/act/Sigmoid, /model.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to model.0.conv.weight + /model.0/conv/_weight_quantizer/QuantizeLinear + /model.0/conv/Conv + PWN(/model.0/act/Sigmoid, /model.0/act/Mul), Location: Device, Dimensions: [1,3,672,672], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.1/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,672,672], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xbd976ef514eaa406
Name: model.1.conv.weight + /model.1/conv/_weight_quantizer/QuantizeLinear + /model.1/conv/Conv + PWN(/model.1/act/Sigmoid, /model.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.1/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,672,672], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.2/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,336,336], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.2.conv.weight + /model.2/conv/_weight_quantizer/QuantizeLinear + /model.2/conv/Conv + PWN(/model.2/act/Sigmoid, /model.2/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.2/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,336,336], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.3/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,336,336], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.3.conv.weight + /model.3/conv/_weight_quantizer/QuantizeLinear + /model.3/conv/Conv + PWN(/model.3/act/Sigmoid, /model.3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.3/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,336,336], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.4/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xafad4a0ea10d6400
Name: model.4.conv.weight + /model.4/conv/_weight_quantizer/QuantizeLinear + /model.4/conv/Conv + PWN(/model.4/act/Sigmoid, /model.4/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.4/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc1a4243b1f1cfe6d
Name: model.5.conv.weight + /model.5/conv/_weight_quantizer/QuantizeLinear + /model.5/conv/Conv + PWN(/model.5/act/Sigmoid, /model.5/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.4/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc1a4243b1f1cfe6d
Name: model.6.conv.weight + /model.6/conv/_weight_quantizer/QuantizeLinear + /model.6/conv/Conv + PWN(/model.6/act/Sigmoid, /model.6/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.7/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.7.conv.weight + /model.7/conv/_weight_quantizer/QuantizeLinear + /model.7/conv/Conv + PWN(/model.7/act/Sigmoid, /model.7/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.7/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.8.conv.weight + /model.8/conv/_weight_quantizer/QuantizeLinear + /model.8/conv/Conv + PWN(/model.8/act/Sigmoid, /model.8/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.9/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.9.conv.weight + /model.9/conv/_weight_quantizer/QuantizeLinear + /model.9/conv/Conv + PWN(/model.9/act/Sigmoid, /model.9/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.9/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x3ac8602b2543f50d
Name: model.11.conv.weight + /model.11/conv/_weight_quantizer/QuantizeLinear + /model.11/conv/Conv + PWN(/model.11/act/Sigmoid, /model.11/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.11/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.12/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x0e1225de5ed2825a
Name: /model.12/m/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.12/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.13/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: model.14.conv.weight + /model.14/conv/_weight_quantizer/QuantizeLinear + /model.14/conv/Conv + PWN(/model.14/act/Sigmoid, /model.14/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.12/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.15/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x0e1225de5ed2825a
Name: model.13.conv.weight + /model.13/conv/_weight_quantizer/QuantizeLinear + /model.13/conv/Conv + PWN(/model.13/act/Sigmoid, /model.13/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.13/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.17/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.15.conv.weight + /model.15/conv/_weight_quantizer/QuantizeLinear + /model.15/conv/Conv + PWN(/model.15/act/Sigmoid, /model.15/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.15/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,168,168], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.17/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0
Name: model.17.conv.weight + /model.17/conv/_weight_quantizer/QuantizeLinear + /model.17/conv/Conv + PWN(/model.17/act/Sigmoid, /model.17/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.17/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.18.conv.weight + /model.18/conv/_weight_quantizer/QuantizeLinear + /model.18/conv/Conv + PWN(/model.18/act/Sigmoid, /model.18/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.17/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.19.conv.weight + /model.19/conv/_weight_quantizer/QuantizeLinear + /model.19/conv/Conv + PWN(/model.19/act/Sigmoid, /model.19/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.20/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0
Name: model.20.conv.weight + /model.20/conv/_weight_quantizer/QuantizeLinear + /model.20/conv/Conv + PWN(/model.20/act/Sigmoid, /model.20/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.20/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0
Name: model.21.conv.weight + /model.21/conv/_weight_quantizer/QuantizeLinear + /model.21/conv/Conv + PWN(/model.21/act/Sigmoid, /model.21/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.22/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0
Name: model.22.conv.weight + /model.22/conv/_weight_quantizer/QuantizeLinear + /model.22/conv/Conv + PWN(/model.22/act/Sigmoid, /model.22/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.22/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0
Name: model.24.conv.weight + /model.24/conv/_weight_quantizer/QuantizeLinear + /model.24/conv/Conv + PWN(/model.24/act/Sigmoid, /model.24/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.24/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.25/m/MaxPool_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x0e1225de5ed2825a
Name: /model.25/m/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.25/m/MaxPool_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.26/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: model.27.conv.weight + /model.27/conv/_weight_quantizer/QuantizeLinear + /model.27/conv/Conv + PWN(/model.27/act/Sigmoid, /model.27/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.25/m/MaxPool_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.28/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257
Name: model.66.conv.weight + /model.66/conv/_weight_quantizer/QuantizeLinear + /model.66/conv/Conv + PWN(/model.66/act/Sigmoid, /model.66/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.25/m/MaxPool_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.68/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.26.conv.weight + /model.26/conv/_weight_quantizer/QuantizeLinear + /model.26/conv/Conv + PWN(/model.26/act/Sigmoid, /model.26/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.26/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.30/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.28.conv.weight + /model.28/conv/_weight_quantizer/QuantizeLinear + /model.28/conv/Conv + PWN(/model.28/act/Sigmoid, /model.28/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.28/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.30/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.30.conv.weight + /model.30/conv/_weight_quantizer/QuantizeLinear + /model.30/conv/Conv + PWN(/model.30/act/Sigmoid, /model.30/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.30/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.31.conv.weight + /model.31/conv/_weight_quantizer/QuantizeLinear + /model.31/conv/Conv + PWN(/model.31/act/Sigmoid, /model.31/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.30/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.32.conv.weight + /model.32/conv/_weight_quantizer/QuantizeLinear + /model.32/conv/Conv + PWN(/model.32/act/Sigmoid, /model.32/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.33/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.33.conv.weight + /model.33/conv/_weight_quantizer/QuantizeLinear + /model.33/conv/Conv + PWN(/model.33/act/Sigmoid, /model.33/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.33/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.34.conv.weight + /model.34/conv/_weight_quantizer/QuantizeLinear + /model.34/conv/Conv + PWN(/model.34/act/Sigmoid, /model.34/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.35/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.35.conv.weight + /model.35/conv/_weight_quantizer/QuantizeLinear + /model.35/conv/Conv + PWN(/model.35/act/Sigmoid, /model.35/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.35/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.37.conv.weight + /model.37/conv/_weight_quantizer/QuantizeLinear + /model.37/conv/Conv + PWN(/model.37/act/Sigmoid, /model.37/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.37/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.38/m/MaxPool_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Int8", "Count": 1048576}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257
Name: /model.38/m/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.38/m/MaxPool_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.39/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: model.40.conv.weight + /model.40/conv/_weight_quantizer/QuantizeLinear + /model.40/conv/Conv + PWN(/model.40/act/Sigmoid, /model.40/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.38/m/MaxPool_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.41/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.54.conv.weight + /model.54/conv/_weight_quantizer/QuantizeLinear + /model.54/conv/Conv + PWN(/model.54/act/Sigmoid, /model.54/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.38/m/MaxPool_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.56/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.39.conv.weight + /model.39/conv/_weight_quantizer/QuantizeLinear + /model.39/conv/Conv + PWN(/model.39/act/Sigmoid, /model.39/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.39/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.43/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.41.conv.weight + /model.41/conv/_weight_quantizer/QuantizeLinear + /model.41/conv/Conv + PWN(/model.41/act/Sigmoid, /model.41/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.41/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.43/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: model.43.conv.weight + /model.43/conv/_weight_quantizer/QuantizeLinear + /model.43/conv/Conv + PWN(/model.43/act/Sigmoid, /model.43/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.43/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7
Name: model.44.conv.weight + /model.44/conv/_weight_quantizer/QuantizeLinear + /model.44/conv/Conv + PWN(/model.44/act/Sigmoid, /model.44/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.43/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7
Name: model.45.conv.weight + /model.45/conv/_weight_quantizer/QuantizeLinear + /model.45/conv/Conv + PWN(/model.45/act/Sigmoid, /model.45/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.46/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: model.46.conv.weight + /model.46/conv/_weight_quantizer/QuantizeLinear + /model.46/conv/Conv + PWN(/model.46/act/Sigmoid, /model.46/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.46/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: model.47.conv.weight + /model.47/conv/_weight_quantizer/QuantizeLinear + /model.47/conv/Conv + PWN(/model.47/act/Sigmoid, /model.47/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.48/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: model.48.conv.weight + /model.48/conv/_weight_quantizer/QuantizeLinear + /model.48/conv/Conv + PWN(/model.48/act/Sigmoid, /model.48/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.48/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: model.50.conv.weight + /model.50/conv/_weight_quantizer/QuantizeLinear + /model.50/conv/Conv + PWN(/model.50/act/Sigmoid, /model.50/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.50/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv1/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Int8", "Count": 1048576}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.51.cv1.conv.weight + /model.51/cv1/conv/_weight_quantizer/QuantizeLinear + /model.51/cv1/conv/Conv + PWN(/model.51/cv1/act/Sigmoid, /model.51/cv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv1/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv3/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.51.cv2.conv.weight + /model.51/cv2/conv/_weight_quantizer/QuantizeLinear + /model.51/cv2/conv/Conv + PWN(/model.51/cv2/act/Sigmoid, /model.51/cv2/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv1/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv7/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.51.cv3.conv.weight + /model.51/cv3/conv/_weight_quantizer/QuantizeLinear + /model.51/cv3/conv/Conv + PWN(/model.51/cv3/act/Sigmoid, /model.51/cv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv3/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv4/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17
Name: model.51.cv4.conv.weight + /model.51/cv4/conv/_weight_quantizer/QuantizeLinear + /model.51/cv4/conv/Conv + PWN(/model.51/cv4/act/Sigmoid, /model.51/cv4/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv4/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/m.0/MaxPool_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: /model.51/m.0/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.51/m.0/MaxPool_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv5/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [5,5], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [2,2], PostPadding: [2,2], Stride: [1,1], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: /model.51/m.1/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.51/m.0/MaxPool_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv5/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [9,9], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [4,4], PostPadding: [4,4], Stride: [1,1], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: /model.51/m.2/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.51/m.0/MaxPool_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv5/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [13,13], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [6,6], PostPadding: [6,6], Stride: [1,1], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: /model.51/m.0/MaxPool_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.51/m.0/MaxPool_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv5/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.51.cv5.conv.weight + /model.51/cv5/conv/_weight_quantizer/QuantizeLinear + /model.51/cv5/conv/Conv + PWN(/model.51/cv5/act/Sigmoid, /model.51/cv5/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv5/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,2048,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv6/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.51.cv6.conv.weight + /model.51/cv6/conv/_weight_quantizer/QuantizeLinear + /model.51/cv6/conv/Conv + PWN(/model.51/cv6/act/Sigmoid, /model.51/cv6/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv6/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv7/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17
Name: model.51.cv7.conv.weight + /model.51/cv7/conv/_weight_quantizer/QuantizeLinear + /model.51/cv7/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model.51/cv7/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.51/cv7/conv/Conv_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e
Name: PWN(/model.51/cv7/act/Sigmoid, /model.51/cv7/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model.51/cv7/conv/Conv_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], Outputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000018
Name: /model.52/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.52/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8
Name: model.52.conv.weight + /model.52/conv/_weight_quantizer/QuantizeLinear + /model.52/conv/Conv + PWN(/model.52/act/Sigmoid, /model.52/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.52/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.53/Resize_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01
Name: /model.53/Resize, LayerType: Resize, Inputs: [ { Name: /model.53/Resize_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.55/Concat_/model.53/Resize_output_0_clone_1, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Resize, ResizeMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000003
Name: /model.55/Concat_/model.53/Resize_output_0_clone_1 copy, LayerType: Reformat, Inputs: [ { Name: /model.55/Concat_/model.53/Resize_output_0_clone_1, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.56/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.56.conv.weight + /model.56/conv/_weight_quantizer/QuantizeLinear + /model.56/conv/Conv + PWN(/model.56/act/Sigmoid, /model.56/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.56/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.57.conv.weight + /model.57/conv/_weight_quantizer/QuantizeLinear + /model.57/conv/Conv + PWN(/model.57/act/Sigmoid, /model.57/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.56/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.58/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.58.conv.weight + /model.58/conv/_weight_quantizer/QuantizeLinear + /model.58/conv/Conv + PWN(/model.58/act/Sigmoid, /model.58/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.58/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.59/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: model.59.conv.weight + /model.59/conv/_weight_quantizer/QuantizeLinear + /model.59/conv/Conv + PWN(/model.59/act/Sigmoid, /model.59/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.59/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.60/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: model.60.conv.weight + /model.60/conv/_weight_quantizer/QuantizeLinear + /model.60/conv/Conv + PWN(/model.60/act/Sigmoid, /model.60/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.60/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.61/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: model.61.conv.weight + /model.61/conv/_weight_quantizer/QuantizeLinear + /model.61/conv/Conv + PWN(/model.61/act/Sigmoid, /model.61/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.61/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: /model.61/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.61/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.60/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.60/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.59/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.59/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.58/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.58/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.63.conv.weight + /model.63/conv/_weight_quantizer/QuantizeLinear + /model.63/conv/Conv + PWN(/model.63/act/Sigmoid, /model.63/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.63/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.64.conv.weight + /model.64/conv/_weight_quantizer/QuantizeLinear + /model.64/conv/Conv + PWN(/model.64/act/Sigmoid, /model.64/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.65/Resize_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: /model.65/Resize, LayerType: Resize, Inputs: [ { Name: /model.65/Resize_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.67/Concat_/model.65/Resize_output_0_clone_1, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Resize, ResizeMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005
Name: /model.67/Concat_/model.65/Resize_output_0_clone_1 copy, LayerType: Reformat, Inputs: [ { Name: /model.67/Concat_/model.65/Resize_output_0_clone_1, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.68/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.68.conv.weight + /model.68/conv/_weight_quantizer/QuantizeLinear + /model.68/conv/Conv + PWN(/model.68/act/Sigmoid, /model.68/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.68/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.69.conv.weight + /model.69/conv/_weight_quantizer/QuantizeLinear + /model.69/conv/Conv + PWN(/model.69/act/Sigmoid, /model.69/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.68/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.70/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.70.conv.weight + /model.70/conv/_weight_quantizer/QuantizeLinear + /model.70/conv/Conv + PWN(/model.70/act/Sigmoid, /model.70/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.70/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.71/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.71.conv.weight + /model.71/conv/_weight_quantizer/QuantizeLinear + /model.71/conv/Conv + PWN(/model.71/act/Sigmoid, /model.71/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.71/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.72/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.72.conv.weight + /model.72/conv/_weight_quantizer/QuantizeLinear + /model.72/conv/Conv + PWN(/model.72/act/Sigmoid, /model.72/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.72/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.73/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: model.73.conv.weight + /model.73/conv/_weight_quantizer/QuantizeLinear + /model.73/conv/Conv + PWN(/model.73/act/Sigmoid, /model.73/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.73/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84
Name: /model.73/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.73/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.72/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.72/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.71/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.71/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.70/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.70/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.75.conv.weight + /model.75/conv/_weight_quantizer/QuantizeLinear + /model.75/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model.75/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.75/conv/Conv_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f16_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x758f8b2079a95b2e
Name: PWN(/model.75/act/Sigmoid, /model.75/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model.75/conv/Conv_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.75/act/Mul_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000018
Name: /model.77/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.75/act/Mul_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.76/m/MaxPool_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.102/rbr_reparam/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.75/act/Mul_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.102/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.76/m/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.76/m/MaxPool_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.77/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: model.78.conv.weight + /model.78/conv/_weight_quantizer/QuantizeLinear + /model.78/conv/Conv + PWN(/model.78/act/Sigmoid, /model.78/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.76/m/MaxPool_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.79/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 16384}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c
Name: model.102.rbr_reparam.weight + /model.102/rbr_reparam/_weight_quantizer/QuantizeLinear + /model.102/rbr_reparam/Conv + PWN(/model.102/act/Sigmoid, /model.102/act/Mul), LayerType: CaskJitConv, Inputs: [ { Name: /model.102/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.102/act/Mul_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: GENERIC, ParameterSubType: PointWiseExpression, NbPointWiseExpressionInputArgs: 1, PointWiseExpressionInputArgs: ["arg0"], NbPointWiseExpressionOutputVars: 1, PointWiseExpressionOutputVars: ["var4"], NbPointWiseExpressionParams: 0, PointWiseExpressionParams: [], NbPointWiseExpressionLiterals: 5, PointWiseExpressionLiterals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbPointWiseExpressionOperations: 5, PointWiseExpressionOperations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], HasBias: 1, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd3d41ef6de22d9b6
Name: model.77.conv.weight + /model.77/conv/_weight_quantizer/QuantizeLinear + /model.77/conv/Conv + PWN(/model.77/act/Sigmoid, /model.77/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.77/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 16384}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.79.conv.weight + /model.79/conv/_weight_quantizer/QuantizeLinear + /model.79/conv/Conv + PWN(/model.79/act/Sigmoid, /model.79/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.79/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,84,84], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: Reformatting CopyNode for Input Tensor 0 to /model.105/m.0/Conv, LayerType: Reformat, Inputs: [ { Name: /model.102/act/Mul_output_0, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.0/Conv, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003ea
Name: /model.105/m.0/Conv, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.0/Conv, Location: Device, Dimensions: [1,256,84,84], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: /model.105/m.0/Conv_output_0, Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 255, Groups: 1, Weights: {"Type": "Half", "Count": 65280}, Bias: {"Type": "Half", "Count": 255}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x16_simple_t1r1s1, TacticValue: 0xa2121200e08f5391
Name: Reformatting CopyNode for Input Tensor 0 to PWN(/model.105/Sigmoid), LayerType: Reformat, Inputs: [ { Name: /model.105/m.0/Conv_output_0, Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid), Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: PWN(/model.105/Sigmoid), LayerType: PointWiseV2, Inputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid), Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Transpose_output_0, Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Row major linear FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var3"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 4, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);"], TacticValue: 0x0000000000000005
Name: model.81.conv.weight + /model.81/conv/_weight_quantizer/QuantizeLinear + /model.81/conv/Conv + PWN(/model.81/act/Sigmoid, /model.81/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.82.conv.weight + /model.82/conv/_weight_quantizer/QuantizeLinear + /model.82/conv/Conv + PWN(/model.82/act/Sigmoid, /model.82/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.81/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.83/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: /model.105/Reshape + /model.105/Transpose, LayerType: Shuffle, Inputs: [ { Name: /model.105/Transpose_output_0, Location: Device, Dimensions: [1,255,84,84], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Sigmoid_output_0, Location: Device, Dimensions: [1,3,84,84,85], Format/Datatype: Row major linear FP16 format }], ParameterType: Shuffle, FirstTranspose: [0,1,2,3], Reshape: [1,3,85,84,84], SecondTranspose: [0,1,3,4,2], ZeroIsPlaceholder: 1, TacticValue: 0x0000000000000000
Name: model.83.conv.weight + /model.83/conv/_weight_quantizer/QuantizeLinear + /model.83/conv/Conv + PWN(/model.83/act/Sigmoid, /model.83/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.83/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.84/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: /model.105/Split, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_output_0, Location: Device, Dimensions: [1,3,84,84,85], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Split_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Slice, Start: [0,0,0,0,0], Size: [1,3,84,84,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_179, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_output_0, Location: Device, Dimensions: [1,3,84,84,85], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Split_output_1, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Slice, Start: [0,0,0,0,2], Size: [1,3,84,84,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_181, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_output_0, Location: Device, Dimensions: [1,3,84,84,85], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Concat_1_output_0, Location: Device, Dimensions: [1,3,84,84,81], Format/Datatype: Row major linear FP16 format }], ParameterType: Slice, Start: [0,0,0,0,4], Size: [1,3,84,84,81], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: model.84.conv.weight + /model.84/conv/_weight_quantizer/QuantizeLinear + /model.84/conv/Conv + PWN(/model.84/act/Sigmoid, /model.84/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.84/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.85/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: PWN(/model.105/Constant_5_output_0 + (Unnamed Layer* 1089) [Shuffle] + /model.105/Mul, /model.105/Add), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }, { Name: (Unnamed Layer* 1091) [Constant]_output, Location: Device, Dimensions: [1,1,84,84,2], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Add_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 2, Literals: ["0.000000e+00f", "1.600000e+01f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iMul(arg0, literal1);", "auto const var1 = pwgen::iPlus(var0, arg1);"], TacticValue: 0x0000000000000000
Name: PWN(PWN(/model.105/Constant_7_output_0 + (Unnamed Layer* 1094) [Shuffle], /model.105/Pow), /model.105/Mul_1), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_output_1, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }, { Name: (Unnamed Layer* 1096) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Mul_1_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 1, Literals: ["2.000000e+00f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iPow(arg0, literal0);", "auto const var1 = pwgen::iMul(var0, arg1);"], TacticValue: 0x000000000000001c
Name: model.85.conv.weight + /model.85/conv/_weight_quantizer/QuantizeLinear + /model.85/conv/Conv + PWN(/model.85/act/Sigmoid, /model.85/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.85/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.86/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: /model.105/Add_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Add_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_1_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: /model.105/Mul_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Mul_1_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_1_output_0, Location: Device, Dimensions: [1,3,84,84,2], Format/Datatype: Row major linear FP16 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.86.conv.weight + /model.86/conv/_weight_quantizer/QuantizeLinear + /model.86/conv/Conv + PWN(/model.86/act/Sigmoid, /model.86/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.86/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237
Name: /model.105/Reshape_1, LayerType: NoOp, Inputs: [ { Name: /model.105/Concat_1_output_0, Location: Device, Dimensions: [1,3,84,84,85], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Reshape_1_copy_output, Location: Device, Dimensions: [1,21168,85], Format/Datatype: Row major linear FP16 format }], TacticValue: 0x0000000000000000
Name: /model.105/Reshape_1_copy_output, LayerType: Reformat, Inputs: [ { Name: /model.105/Reshape_1_copy_output, Location: Device, Dimensions: [1,21168,85], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: outputs, Location: Device, Dimensions: [1,21168,85], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: /model.86/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.86/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.85/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.85/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.84/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.84/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.83/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.83/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.88.conv.weight + /model.88/conv/_weight_quantizer/QuantizeLinear + /model.88/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model.88/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.88/conv/Conv_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 262144}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f16_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x8e1dd2962c589dd4
Name: PWN(/model.88/act/Sigmoid, /model.88/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model.88/conv/Conv_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.88/act/Mul_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000014
Name: /model.90/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.88/act/Mul_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.89/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.103/rbr_reparam/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.88/act/Mul_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.103/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.89/m/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model.89/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.90/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba
Name: model.91.conv.weight + /model.91/conv/_weight_quantizer/QuantizeLinear + /model.91/conv/Conv + PWN(/model.91/act/Sigmoid, /model.91/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.89/m/MaxPool_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.92/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.103.rbr_reparam.weight + /model.103/rbr_reparam/_weight_quantizer/QuantizeLinear + /model.103/rbr_reparam/Conv + PWN(/model.103/act/Sigmoid, /model.103/act/Mul), LayerType: CaskJitConv, Inputs: [ { Name: /model.103/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.103/act/Mul_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: GENERIC, ParameterSubType: PointWiseExpression, NbPointWiseExpressionInputArgs: 1, PointWiseExpressionInputArgs: ["arg0"], NbPointWiseExpressionOutputVars: 1, PointWiseExpressionOutputVars: ["var4"], NbPointWiseExpressionParams: 0, PointWiseExpressionParams: [], NbPointWiseExpressionLiterals: 5, PointWiseExpressionLiterals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbPointWiseExpressionOperations: 5, PointWiseExpressionOperations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], HasBias: 1, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b
Name: model.90.conv.weight + /model.90/conv/_weight_quantizer/QuantizeLinear + /model.90/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model.90/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.90/conv/Conv_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190
Name: model.92.conv.weight + /model.92/conv/_weight_quantizer/QuantizeLinear + /model.92/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model.92/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,42,42], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.92/conv/Conv_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e
Name: Reformatting CopyNode for Input Tensor 0 to /model.105/m.1/Conv, LayerType: Reformat, Inputs: [ { Name: /model.103/act/Mul_output_0, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.1/Conv, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x00000000000003e8
Name: /model.105/m.1/Conv, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.1/Conv, Location: Device, Dimensions: [1,512,42,42], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: /model.105/m.1/Conv_output_0, Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 255, Groups: 1, Weights: {"Type": "Half", "Count": 130560}, Bias: {"Type": "Half", "Count": 255}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x16_simple_t1r1s1, TacticValue: 0x361d222e42fcb76c
Name: PWN(/model.90/act/Sigmoid, /model.90/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model.90/conv/Conv_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], Outputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x000000000000001f
Name: PWN(/model.92/act/Sigmoid, /model.92/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model.92/conv/Conv_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP32 format }], Outputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x000000000000001f
Name: Reformatting CopyNode for Input Tensor 0 to PWN(/model.105/Sigmoid_1), LayerType: Reformat, Inputs: [ { Name: /model.105/m.1/Conv_output_0, Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid_1), Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: PWN(/model.105/Sigmoid_1), LayerType: PointWiseV2, Inputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid_1), Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Transpose_1_output_0, Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var3"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 4, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);"], TacticValue: 0x0000000000000009
Name: /model.105/Reshape_2 + /model.105/Transpose_1, LayerType: Shuffle, Inputs: [ { Name: /model.105/Transpose_1_output_0, Location: Device, Dimensions: [1,255,42,42], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Sigmoid_1_output_0, Location: Device, Dimensions: [1,3,42,42,85], Format/Datatype: Row major linear FP32 }], ParameterType: Shuffle, FirstTranspose: [0,1,2,3], Reshape: [1,3,85,42,42], SecondTranspose: [0,1,3,4,2], ZeroIsPlaceholder: 1, TacticValue: 0x0000000000000000
Name: /model.94/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.94/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.95/conv/_input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model.93/Concat_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: /model.95/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea
Name: /model.105/Split_1, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_1_output_0, Location: Device, Dimensions: [1,3,42,42,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Split_1_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,0], Size: [1,3,42,42,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_1_196, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_1_output_0, Location: Device, Dimensions: [1,3,42,42,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Split_1_output_1, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,2], Size: [1,3,42,42,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_1_198, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_1_output_0, Location: Device, Dimensions: [1,3,42,42,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_4_output_0, Location: Device, Dimensions: [1,3,42,42,81], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,4], Size: [1,3,42,42,81], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: model.94.conv.weight + /model.94/conv/_weight_quantizer/QuantizeLinear + /model.94/conv/Conv + PWN(/model.94/act/Sigmoid, /model.94/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.94/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.95.conv.weight + /model.95/conv/_weight_quantizer/QuantizeLinear + /model.95/conv/Conv + PWN(/model.95/act/Sigmoid, /model.95/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.95/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.96/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 524288}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: PWN(/model.105/Constant_16_output_0 + (Unnamed Layer* 1138) [Shuffle] + /model.105/Mul_2, /model.105/Add_1), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_1_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }, { Name: (Unnamed Layer* 1140) [Constant]_output, Location: Device, Dimensions: [1,1,42,42,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Add_1_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 2, Literals: ["0.000000e+00f", "3.200000e+01f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iMul(arg0, literal1);", "auto const var1 = pwgen::iPlus(var0, arg1);"], TacticValue: 0x000000000000001c
Name: PWN(PWN(/model.105/Constant_18_output_0 + (Unnamed Layer* 1143) [Shuffle], /model.105/Pow_1), /model.105/Mul_3), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_1_output_1, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }, { Name: (Unnamed Layer* 1145) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Mul_3_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 1, Literals: ["2.000000e+00f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iPow(arg0, literal0);", "auto const var1 = pwgen::iMul(var0, arg1);"], TacticValue: 0x000000000000001c
Name: model.96.conv.weight + /model.96/conv/_weight_quantizer/QuantizeLinear + /model.96/conv/Conv + PWN(/model.96/act/Sigmoid, /model.96/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.96/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.97/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: /model.105/Add_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Add_1_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Concat_4_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: /model.105/Mul_3_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Mul_3_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_4_output_0, Location: Device, Dimensions: [1,3,42,42,2], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: model.97.conv.weight + /model.97/conv/_weight_quantizer/QuantizeLinear + /model.97/conv/Conv + PWN(/model.97/act/Sigmoid, /model.97/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.97/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.98/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: /model.105/Reshape_3, LayerType: NoOp, Inputs: [ { Name: /model.105/Concat_4_output_0, Location: Device, Dimensions: [1,3,42,42,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Reshape_3_copy_output, Location: Device, Dimensions: [1,5292,85], Format/Datatype: Row major linear FP32 }], TacticValue: 0x0000000000000000
Name: /model.105/Reshape_3_copy_output, LayerType: Reformat, Inputs: [ { Name: /model.105/Reshape_3_copy_output, Location: Device, Dimensions: [1,5292,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: outputs, Location: Device, Dimensions: [1,5292,85], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: model.98.conv.weight + /model.98/conv/_weight_quantizer/QuantizeLinear + /model.98/conv/Conv + PWN(/model.98/act/Sigmoid, /model.98/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.98/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.99/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: model.99.conv.weight + /model.99/conv/_weight_quantizer/QuantizeLinear + /model.99/conv/Conv + PWN(/model.99/act/Sigmoid, /model.99/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.99/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4
Name: /model.99/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.99/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.98/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.98/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.97/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.97/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: /model.96/conv/_input_quantizer/QuantizeLinear_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.96/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000
Name: model.101.conv.weight + /model.101/conv/_weight_quantizer/QuantizeLinear + /model.101/conv/Conv + PWN(/model.101/act/Sigmoid, /model.101/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model.101/conv/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,2048,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.104/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1048576}, Bias: {"Type": "Float", "Count": 512}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3
Name: model.104.rbr_reparam.weight + /model.104/rbr_reparam/_weight_quantizer/QuantizeLinear + /model.104/rbr_reparam/Conv + PWN(/model.104/act/Sigmoid, /model.104/act/Mul), LayerType: CaskJitConv, Inputs: [ { Name: /model.104/rbr_reparam/_input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,21,21], Format/Datatype: Thirty-two wide channel vectorized row major Int8 format }], Outputs: [ { Name: /model.104/act/Mul_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 1024, Groups: 1, Weights: {"Type": "Int8", "Count": 4718592}, Bias: {"Type": "Float", "Count": 1024}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: GENERIC, ParameterSubType: PointWiseExpression, NbPointWiseExpressionInputArgs: 1, PointWiseExpressionInputArgs: ["arg0"], NbPointWiseExpressionOutputVars: 1, PointWiseExpressionOutputVars: ["var4"], NbPointWiseExpressionParams: 0, PointWiseExpressionParams: [], NbPointWiseExpressionLiterals: 5, PointWiseExpressionLiterals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbPointWiseExpressionOperations: 5, PointWiseExpressionOperations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], HasBias: 1, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4
Name: Reformatting CopyNode for Input Tensor 0 to /model.105/m.2/Conv, LayerType: Reformat, Inputs: [ { Name: /model.104/act/Mul_output_0, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Thirty-two wide channel vectorized row major FP16 format }], Outputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.2/Conv, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: /model.105/m.2/Conv, LayerType: CaskConvolution, Inputs: [ { Name: Reformatted Input Tensor 0 to /model.105/m.2/Conv, Location: Device, Dimensions: [1,1024,21,21], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: /model.105/m.2/Conv_output_0, Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 255, Groups: 1, Weights: {"Type": "Half", "Count": 261120}, Bias: {"Type": "Half", "Count": 255}, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, HasBias: 1, HasReLU: 0, TacticName: sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_t1r1s1, TacticValue: 0x2aa016c86360697f
Name: Reformatting CopyNode for Input Tensor 0 to PWN(/model.105/Sigmoid_2), LayerType: Reformat, Inputs: [ { Name: /model.105/m.2/Conv_output_0, Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Channel major FP16 format where channel % 8 == 0 }], Outputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid_2), Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: REFORMAT, TacticValue: 0x0000000000000000
Name: PWN(/model.105/Sigmoid_2), LayerType: PointWiseV2, Inputs: [ { Name: Reformatted Input Tensor 0 to PWN(/model.105/Sigmoid_2), Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Transpose_2_output_0, Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var3"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 4, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);"], TacticValue: 0x0000000000000007
Name: /model.105/Reshape_4 + /model.105/Transpose_2, LayerType: Shuffle, Inputs: [ { Name: /model.105/Transpose_2_output_0, Location: Device, Dimensions: [1,255,21,21], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Sigmoid_2_output_0, Location: Device, Dimensions: [1,3,21,21,85], Format/Datatype: Row major linear FP32 }], ParameterType: Shuffle, FirstTranspose: [0,1,2,3], Reshape: [1,3,85,21,21], SecondTranspose: [0,1,3,4,2], ZeroIsPlaceholder: 1, TacticValue: 0x0000000000000000
Name: /model.105/Split_2, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_2_output_0, Location: Device, Dimensions: [1,3,21,21,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Split_2_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,0], Size: [1,3,21,21,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_2_213, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_2_output_0, Location: Device, Dimensions: [1,3,21,21,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Split_2_output_1, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,2], Size: [1,3,21,21,2], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: /model.105/Split_2_215, LayerType: Slice, Inputs: [ { Name: /model.105/Sigmoid_2_output_0, Location: Device, Dimensions: [1,3,21,21,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_7_output_0, Location: Device, Dimensions: [1,3,21,21,81], Format/Datatype: Row major linear FP32 }], ParameterType: Slice, Start: [0,0,0,0,4], Size: [1,3,21,21,81], Stride: [1,1,1,1,1], Mode: kSTRICT_BOUNDS, negativeInfinityPadding: 0, TacticValue: 0x0000000000000000
Name: PWN(/model.105/Constant_27_output_0 + (Unnamed Layer* 1187) [Shuffle] + /model.105/Mul_4, /model.105/Add_2), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_2_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }, { Name: (Unnamed Layer* 1189) [Constant]_output, Location: Device, Dimensions: [1,1,21,21,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Add_2_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP16 format }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 2, Literals: ["0.000000e+00f", "6.400000e+01f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iMul(arg0, literal1);", "auto const var1 = pwgen::iPlus(var0, arg1);"], TacticValue: 0x0000000000000000
Name: PWN(PWN(/model.105/Constant_29_output_0 + (Unnamed Layer* 1192) [Shuffle], /model.105/Pow_2), /model.105/Mul_5), LayerType: PointWiseV2, Inputs: [ { Name: /model.105/Split_2_output_1, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }, { Name: (Unnamed Layer* 1194) [Constant]_output, Location: Device, Dimensions: [1,3,1,1,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Mul_5_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var1"], NbParams: 0, Params: [], NbLiterals: 1, Literals: ["2.000000e+00f"], NbOperations: 2, Operations: ["auto const var0 = pwgen::iPow(arg0, literal0);", "auto const var1 = pwgen::iMul(var0, arg1);"], TacticValue: 0x000000000000001c
Name: /model.105/Add_2_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Add_2_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP16 format }], Outputs: [ { Name: /model.105/Concat_7_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: /model.105/Mul_5_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model.105/Mul_5_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Concat_7_output_0, Location: Device, Dimensions: [1,3,21,21,2], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8
Name: /model.105/Reshape_5, LayerType: NoOp, Inputs: [ { Name: /model.105/Concat_7_output_0, Location: Device, Dimensions: [1,3,21,21,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: /model.105/Reshape_5_copy_output, Location: Device, Dimensions: [1,1323,85], Format/Datatype: Row major linear FP32 }], TacticValue: 0x0000000000000000
Name: /model.105/Reshape_5_copy_output, LayerType: Reformat, Inputs: [ { Name: /model.105/Reshape_5_copy_output, Location: Device, Dimensions: [1,1323,85], Format/Datatype: Row major linear FP32 }], Outputs: [ { Name: outputs, Location: Device, Dimensions: [1,1323,85], Format/Datatype: Row major linear FP32 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x00000000000003e8

Bindings:
images
outputs
[02/06/2024-23:22:35] [I] Skipped inference phase since --buildOnly is added.
&&&& PASSED TensorRT.trtexec [TensorRT v8503] # trtexec --onnx=experiments/eval_trt/qat_best_5108.onnx --saveEngine=experiments/eval_trt/qat_best_5108.engine --fp16 --int8 --buildOnly --memPoolSize=workspace:1024MiB --dumpLayerInfo --exportLayerInfo=experiments/eval_trt/qat_best_5108.graph --profilingVerbosity=detailed
Engine and graph files generated successfully:
Engine file: experiments/eval_trt/qat_best_5108.engine
Graph file: experiments/eval_trt/qat_best_5108.graph
Created file:///yolov7/experiments/eval_trt/qat_best_5108.graph.svg
Namespace(augment=False, batch_size=1, cfg='cfg/training/yolov7_qat.yaml', conf_thres=0.001, data='data/coco.yaml', device='0', engine='experiments/eval_trt/qat_best_5108.engine', exist_ok=False, hyp='data/hyp.scratch.p5-qat.yaml', img_size=640, iou_thres=0.65, name='exp', no_trace=True, project='runs/test', save_conf=False, save_hybrid=False, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False)
[02/06/2024-23:22:39] [TRT] [I] Loaded engine size: 38 MiB
[02/06/2024-23:22:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +36, now: CPU 0, GPU 36 (MiB)
scripts/eval-trt.py:88: DeprecationWarning: Use get_tensor_shape instead.
  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size
scripts/eval-trt.py:88: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.
  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size
[02/06/2024-23:22:39] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
scripts/eval-trt.py:89: DeprecationWarning: Use get_tensor_shape instead.
  print("binding shape: ", engine.get_binding_shape(binding))
binding shape:  (1, 3, 672, 672)
scripts/eval-trt.py:90: DeprecationWarning: Use get_tensor_dtype instead.
  dtype = trt.nptype(engine.get_binding_dtype(binding))
scripts/eval-trt.py:97: DeprecationWarning: Use get_tensor_mode instead.
  if engine.binding_is_input(binding):
[02/06/2024-23:22:39] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
binding shape:  (1, 27783, 85)
scripts/eval-trt.py:143: DeprecationWarning: Use get_tensor_shape instead.
  outputshape = [engine.get_binding_shape(binding) for binding in engine][1]
[02/06/2024-23:22:39] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +32, now: CPU 0, GPU 68 (MiB)
val: Scanning 'coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100%|| 5000/5000 [00:00<?, ?it/s]
               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|| 5000/5000 [00:53<00:00, 94.34it/s]
                 all        5000       36335     0.74385     0.62162     0.68921     0.49638
Speed: 3.4/1.1/4.5 ms inference/NMS/total per 640x640 image at batch-size 1

Evaluating pycocotools mAP... saving qat_best_5108_predictions.json...
loading annotations into memory...
Done (t=0.42s)
creating index...
index created!
Loading and preparing results...
DONE (t=2.91s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=49.62s).
Accumulating evaluation results...
DONE (t=7.16s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.511
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.696
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.553
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.352
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.560
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.666
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.384
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.636
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.687
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.535
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.736
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.838
